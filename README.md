# Anomalous Market Behavior Recognition with Machine Learning

[![CI](https://github.com/jamesolaitan/Anomalous-Market-Behavior-Recognition-with-Machine-Learning/workflows/CI/badge.svg)](https://github.com/jamesolaitan/Anomalous-Market-Behavior-Recognition-with-Machine-Learning/actions)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> **Detect correlation breakdown anomalies in financial markets using LSTM neural networks with Markov temporal smoothing**

A production-ready machine learning pipeline for detecting anomalous market behavior through correlation breakdown analysis. This project combines PyTorch LSTM models with a simple Markov (HMM-lite) temporal smoother to identify periods when asset correlations deviate significantly from normal patterns.

## ğŸ¯ Key Features

- **LSTM Anomaly Detection**: PyTorch-based LSTM model trained on ~14 years of financial data
- **VIX Integration**: Incorporates VIX (volatility index) as a systemic risk feature
- **Markov Temporal Smoother**: Simple 2-3 state HMM-lite reduces false positives via temporal smoothing
- **SQL Data Pipeline**: DuckDB-based pipeline for efficient data processing and storage
- **Interactive Dashboard**: Streamlit dashboard for real-time exploration and visualization
- **Production Ready**: Complete testing, CI/CD, Docker support, and comprehensive documentation

## ğŸ“Š Current Performance Metrics

| Model | Precision | Recall | **F1 Score** | ROC-AUC | PR-AUC |
|-------|-----------|--------|--------------|---------|--------|
| LSTM  | 0.750       | 0.600    | **0.667**      | 1.000     | 0.787    |
| Markov| 0.000       | 0.000    | **0.000**      | 1.000     | 0.787    |

*Note: Metrics will be computed after running `make all`. See [Running the Pipeline](#-running-the-pipeline) below.*

**Last Updated**: 2025-10-07 10:13 UTC *(commit: `cf28c22`)*
**Dataset**: 2010-2024 (SPY, XLF, XLK, VNQ + VIX)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data      â”‚â”€â”€â”€â”€â–¶â”‚   Features   â”‚â”€â”€â”€â”€â–¶â”‚   Labels    â”‚
â”‚ (yfinance)  â”‚     â”‚ (rolling corrâ”‚     â”‚(breakdown)  â”‚
â”‚ SPY,XLF,    â”‚     â”‚  vol, VIX)   â”‚     â”‚   rules     â”‚
â”‚ XLK,VNQ,VIX â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚                    â”‚
                             â–¼                    â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚       DuckDB Database        â”‚
                      â”‚  (raw_prices, features,      â”‚
                      â”‚   labels, predictions)       â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â–¼                           â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ LSTM Model   â”‚          â”‚   Markov     â”‚
              â”‚  (PyTorch)   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  Smoother    â”‚
              â”‚              â”‚ p_anom   â”‚  (HMM-lite)  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚                           â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚    Evaluation Pipeline   â”‚
                      â”‚  (Metrics, Plots, JSON)  â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚  Streamlit Dashboard     â”‚
                      â”‚  (Interactive Viz)       â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
repo/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ download.py         # Fetch OHLCV + VIX from Yahoo Finance
â”‚   â”‚   â”œâ”€â”€ ingest_sql.py       # Load data into DuckDB
â”‚   â”‚   â””â”€â”€ features.py         # Feature engineering (corr, vol, VIX)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ lstm.py             # PyTorch LSTM model
â”‚   â”‚   â”œâ”€â”€ markov_smoother.py  # HMM-lite temporal smoother
â”‚   â”‚   â””â”€â”€ thresholds.py       # Threshold tuning utilities
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”œâ”€â”€ train.py            # Training orchestration
â”‚   â”‚   â”œâ”€â”€ predict.py          # Prediction generation
â”‚   â”‚   â””â”€â”€ evaluate.py         # Metrics & plots
â”‚   â”œâ”€â”€ viz/
â”‚   â”‚   â””â”€â”€ dashboard.py        # Streamlit dashboard
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config.py           # Configuration management
â”‚       â”œâ”€â”€ seed.py             # Random seed utilities
â”‚       â”œâ”€â”€ logging_config.py   # Logging setup
â”‚       â””â”€â”€ io.py               # I/O utilities
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ schema.sql              # Database schema
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_features.py        # Feature engineering tests
â”‚   â”œâ”€â”€ test_lstm.py            # LSTM model tests
â”‚   â”œâ”€â”€ test_markov.py          # Markov smoother tests
â”‚   â””â”€â”€ test_end_to_end.py      # Integration tests
â”œâ”€â”€ config.yaml                 # Main configuration file
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ Makefile                    # Build automation
â”œâ”€â”€ Dockerfile                  # Container definition
â””â”€â”€ README.md                   # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9+
- pip

### Installation

```bash
# Clone the repository
git clone https://github.com/jamesolaitan/Anomalous-Market-Behavior-Recognition-with-Machine-Learning.git
cd Anomalous-Market-Behavior-Recognition-with-Machine-Learning

# Install dependencies
make setup

# Or manually:
pip install -r requirements.txt
pre-commit install
```

### ğŸ”§ Running the Pipeline

Run the complete pipeline with a single command:

```bash
make all
```

This executes:
1. **Data download** (`make data`): Fetches ~14 years of data from Yahoo Finance
2. **Feature engineering** (`make features`): Computes rolling correlations, volatility, z-scores, VIX features
3. **Training** (`make train`): Trains LSTM model with early stopping (~100 epochs)
4. **Prediction** (`make predict`): Generates predictions and applies Markov smoothing
5. **Evaluation** (`make eval`): Computes metrics (F1, ROC-AUC, PR-AUC) and generates plots

Or run steps individually:

```bash
make data         # Download and ingest data
make features     # Engineer features
make train        # Train LSTM model
make predict      # Generate predictions
make eval         # Evaluate and compute metrics
make dashboard    # Launch Streamlit dashboard
```

### ğŸ“Š View Results

After running the pipeline, launch the interactive dashboard:

```bash
make dashboard
# or
streamlit run src/viz/dashboard.py
```

Open your browser to `http://localhost:8501` to explore:
- Time series with anomaly highlights
- LSTM anomaly probabilities
- Markov smoothed posteriors
- Feature correlations and VIX
- Performance metrics

### ğŸ§ª Testing

```bash
make test         # Run all tests with coverage
make lint         # Run linters (flake8, mypy)
make format       # Format code (black, isort)
```

## ğŸ“– How It Works

### 1. Data & Features

**Symbols**: SPY (S&P 500), XLF (Financials), XLK (Technology), VNQ (Real Estate)  
**Period**: 2010-01-01 to 2024-12-31  
**Features**:
- Daily returns and log returns
- Rolling volatility (20-day window)
- Rolling correlation with SPY (60-day window)
- Z-scores of correlation and volatility
- VIX level and delta

### 2. Labeling (Correlation Breakdown)

Anomalies are labeled when:
1. Rolling correlation drops below threshold (< 0.1)
2. **AND** correlation change is steep (Î”corr < -0.3 within 10 days)

Labels persist for 5 days to mark "breakdown episodes."

### 3. LSTM Model

**Architecture**:
- Input: 8 features
- LSTM: 1 layer, 64 hidden units
- Output: sigmoid(logit) â†’ P(anomaly)

**Training**:
- Loss: BCEWithLogitsLoss with pos_weight=5.0 (class imbalance)
- Optimizer: Adam (lr=0.001)
- Early stopping: patience=10 epochs on validation PR-AUC

### 4. Markov Smoother (HMM-lite)

**States**: Normal (N), Anomalous (A)

**Transition Matrix** (default):
```
        N      A
N    0.97   0.03
A    0.15   0.85
```

**How it works**:
1. Takes LSTM probabilities `p_anom_t` as observations
2. Performs forward update: `prior_t = posterior_{t-1} @ T`
3. Computes posterior: `post_t âˆ prior_t * P(obs_t | state)`
4. Flags anomaly if `P(A) > 0.7` for 3+ consecutive steps

**Benefits**: Reduces false positives by enforcing temporal persistence.

### 5. Evaluation

**Metrics** (both point-level and event-level):
- Precision, Recall, F1 Score
- ROC-AUC
- PR-AUC (Precision-Recall Area Under Curve)

**Outputs**:
- `artifacts/metrics.json`: Computed metrics
- `artifacts/plots/`: ROC curves, PR curves, confusion matrices, time series

## âš™ï¸ Configuration

All parameters are in `config.yaml`:

```yaml
data:
  symbols: [SPY, XLF, XLK, VNQ]
  start_date: "2010-01-01"
  end_date: "2024-12-31"

features:
  rolling_window: 60  # correlation window

labels:
  corr_threshold: 0.1
  delta_threshold: -0.3

model:
  hidden_size: 64
  learning_rate: 0.001
  epochs: 100

markov:
  num_states: 2
  decision_threshold: 0.7
  consecutive_steps: 3
```

## ğŸ³ Docker

Run the entire pipeline in a reproducible Docker container:

```bash
# Build image
docker build -t anomaly-detection .

# Run pipeline
docker run -v $(pwd)/data:/app/data \
           -v $(pwd)/models:/app/models \
           -v $(pwd)/artifacts:/app/artifacts \
           anomaly-detection \
           make all

# Launch dashboard
docker run -p 8501:8501 \
           -v $(pwd)/data:/app/data \
           -v $(pwd)/models:/app/models \
           -v $(pwd)/artifacts:/app/artifacts \
           anomaly-detection \
           streamlit run src/viz/dashboard.py
```

## ğŸ§© Extending the Project

### Add New Symbols

Edit `config.yaml`:
```yaml
data:
  symbols: [SPY, XLF, XLK, VNQ, QQQ, IWM]
```

### Use 3-State Markov Model

Edit `config.yaml`:
```yaml
markov:
  num_states: 3  # N, A, R (Recovery)
```

### Tune Hyperparameters

Modify `config.yaml` or override via CLI (future enhancement).

## ğŸ“ˆ Results Interpretation

### What is a "Correlation Breakdown"?

Financial assets often move together (correlation). A **correlation breakdown** occurs when this relationship suddenly weakens, often signaling:
- Market stress or regime change
- Sector rotation
- Flight to safety (VIX spike)
- Structural market shifts

### When to Use This Model?

- **Risk monitoring**: Flag periods of unusual market behavior
- **Portfolio rebalancing**: Adjust allocations when correlations break
- **Event detection**: Identify crisis periods (COVID-19, 2008 crash, etc.)

## ğŸ› ï¸ Development

### Adding a New Feature

1. Add computation in `src/data/features.py`
2. Update SQL schema in `sql/schema.sql`
3. Retrain model: `make train`
4. Add tests in `tests/test_features.py`

### Adding a New Model

1. Create module in `src/models/`
2. Update training pipeline in `src/pipelines/train.py`
3. Add tests in `tests/`

## ğŸ“‹ Project Checklist

- âœ… Audit repo: list files, note issues, TODOs
- âœ… Set up tooling: .gitignore, requirements.txt, pre-commit, Makefile, CI
- âœ… Restructure project with proper src/ layout
- âœ… Create utility modules (config, logging, seed, io)
- âœ… Data ingestion (prices + VIX) â†’ SQL with DuckDB
- âœ… Feature engineering (rolling corr, vol, VIX merges, z-scores)
- âœ… Labeling (correlation breakdown rules)
- âœ… LSTM model (PyTorch) - train & save
- âœ… Markov smoother (T estimation + forward update + decision)
- âœ… Prediction pipeline â†’ SQL predictions
- âœ… Evaluation (metrics: P/R/F1, ROC-AUC, PR-AUC; write F1)
- âœ… Dashboard (Streamlit) with visualizations
- âœ… SQL schema and feature views
- âœ… Tests (unit + e2e) and fix failures
- âœ… Docker setup for reproducibility
- âœ… Docs (README with usage, diagrams)
- âœ… GitHub Actions CI

## ğŸ“ Changelog

### Version 2.0.0 (2024)

**Major Upgrade - Complete Rewrite**

- âœ¨ **LSTM Model**: Replaced autoencoder with PyTorch LSTM
- âœ¨ **Markov Smoother**: Added HMM-lite temporal smoothing (2-state)
- âœ¨ **DuckDB Pipeline**: Complete SQL pipeline for data management
- âœ¨ **Streamlit Dashboard**: Interactive visualization dashboard
- âœ¨ **Comprehensive Testing**: Unit tests + e2e tests + CI/CD
- âœ¨ **Production Ready**: Docker, pre-commit hooks, linting, type hints
- ğŸ”§ **Improved Features**: Added VIX delta, z-scores, rolling windows
- ğŸ”§ **Better Labeling**: Correlation breakdown with persistence
- ğŸ”§ **Reproducibility**: Fixed seeds, config-driven, documented
- ğŸ“š **Documentation**: Complete README with architecture diagrams

### Version 1.0.0 (Original)

- Basic autoencoder for anomaly detection
- Simple data loading with yfinance
- TensorFlow/Keras implementation

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

**Before submitting**:
- Run tests: `make test`
- Format code: `make format`
- Check linting: `make lint`

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“§ Contact

**Author**: James Olaitan  
**GitHub**: [@jamesolaitan](https://github.com/jamesolaitan)

## ğŸ™ Acknowledgments

- Data source: [Yahoo Finance](https://finance.yahoo.com/) via `yfinance`
- Frameworks: PyTorch, DuckDB, Streamlit, scikit-learn
- Inspiration: Financial time series analysis and anomaly detection research

---

**â­ If you find this project useful, please consider giving it a star!**
